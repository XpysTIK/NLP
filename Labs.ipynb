{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание №1 \n",
    "\n",
    "$ pip install yargy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yargy import Parser, rule, and_, not_,or_\n",
    "from yargy.interpretation import fact\n",
    "from yargy.relations import gnc_relation\n",
    "from yargy.pipelines import morph_pipeline,pipeline\n",
    "from yargy.predicates import gram, is_capitalized, dictionary, caseless, normalized, gte, lte\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file(url, path):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  \n",
    "        with open(path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Скачал файл с {url} к {path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Ошибка загрузки файла с {url}: {e}\")\n",
    "\n",
    "url = 'https://github.com/XpysTIK/main.git'\n",
    "path = 'news.txt.gz'\n",
    "download_file(url, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterator, List\n",
    "\n",
    "@dataclass\n",
    "class Text:\n",
    "    label: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "def read_texts(fn: str) -> List[Text]:\n",
    "    texts = []\n",
    "    with gzip.open(fn, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            label, title, text = line.strip().split(\"\\t\")\n",
    "            texts.append(Text(label, title, text))\n",
    "    return texts\n",
    "\n",
    "texts = read_texts(\"news.txt.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yargy import Parser, rule, and_, gte, lte, morph_pipeline, gram, normalized, is_capitalized, or_\n",
    "from yargy.interpretation import fact\n",
    "from yargy.predicates import gram\n",
    "\n",
    "Person = fact(\"Person\", [\"name\", \"birth_date\", \"birth_place\"])\n",
    "Name = fact(\"Name\", [\"first\", \"last\"])\n",
    "Birth_date = fact('Birth_date', ['day', 'month', 'year'])\n",
    "Birth_place = fact('Birth_place', ['place'])\n",
    "\n",
    "Day = and_(gte(1), lte(31))\n",
    "\n",
    "Month = morph_pipeline([\n",
    "    \"Январь\", \"Февраль\", \"Март\", \"Апрель\", \"Май\", \"Июнь\", \"Июль\", \"Август\", \"Сентябрь\", \"Октябрь\", \"Ноябрь\", \"Декабрь\"\n",
    "])\n",
    "\n",
    "Year = and_(gte(1), lte(2023))\n",
    "\n",
    "NAME = rule(\n",
    "    gram(\"Имя\").interpretation(Name.first.inflected()),\n",
    "    gram(\"Фамилия\").interpretation(Name.last.inflected())\n",
    ").interpretation(Name)\n",
    "\n",
    "BIRTH_DATE = rule(\n",
    "    Day.interpretation(Birth_date.day).optional(),\n",
    "    Month.interpretation(Birth_date.month).optional(),\n",
    "    normalized('в').optional(),\n",
    "    Year.interpretation(Birth_date.year),\n",
    "    normalized('Год').optional()\n",
    ").interpretation(Birth_date).optional()\n",
    "\n",
    "BIRTH_PLACE = rule(\n",
    "    normalized('в'),\n",
    "    is_capitalized().interpretation(Birth_place.place)\n",
    ").interpretation(Birth_place).optional()\n",
    "\n",
    "BIRTH_DATE_OR_PLACE = or_(\n",
    "    BIRTH_DATE.interpretation(Person.birth_date),\n",
    "    BIRTH_PLACE.interpretation(Person.birth_place)\n",
    ")\n",
    "\n",
    "PERSON = rule(\n",
    "    NAME.interpretation(Person.name),\n",
    "    normalized('Родился'),\n",
    "    BIRTH_DATE_OR_PLACE,\n",
    "    BIRTH_DATE_OR_PLACE\n",
    ").interpretation(Person)\n",
    "\n",
    "parser = Parser(PERSON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for text in tqdm(texts, disable=False):\n",
    "  for match in parser.findall(text.text):\n",
    "    print(match.fact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание №2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "url = 'https://github.com/XpysTIK/main.git'\n",
    "path = 'news.txt.gz'\n",
    "download_file(url, path)\n",
    "\n",
    "tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.save(\"word2vec_model\")\n",
    "\n",
    "word_vector = model.wv['авто']\n",
    "print(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "url = 'https://github.com/XpysTIK/main.git'\n",
    "path = 'news.txt.gz'\n",
    "download_file(url, path)\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenized_train_texts = [word_tokenize(text.lower()) for text in train_texts]\n",
    "tokenized_test_texts = [word_tokenize(text.lower()) for text in test_texts]\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_train_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def document_vector(model, doc):\n",
    "    vectors = [model.wv[word] for word in doc if word in model.wv]\n",
    "    if not vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "train_vectors = [document_vector(model, doc) for doc in tokenized_train_texts]\n",
    "\n",
    "test_vectors = [document_vector(model, doc) for doc in tokenized_test_texts]\n",
    "\n",
    "for i, vector in enumerate(train_vectors):\n",
    "    print(f\"Train Document {i+1} Vector: {vector}\")\n",
    "\n",
    "for i, vector in enumerate(test_vectors):\n",
    "    print(f\"Test Document {i+1} Vector: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X_train = train_vectors\n",
    "y_train = train_labels\n",
    "X_test = test_vectors\n",
    "y_test = test_labels\n",
    "\n",
    "classifier = svm.SVC(kernel='linear')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Точность: {accuracy}\")\n",
    "print(\"Классификационный отчет:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.4\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "url = 'https://github.com/XpysTIK/main.git'\n",
    "path = 'news.txt.gz'\n",
    "download_file(url, path)\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenize, lowercase=True)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_texts)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_texts)\n",
    "\n",
    "classifier_tfidf = SVC(kernel='linear')\n",
    "classifier_tfidf.fit(X_train_tfidf, train_labels)\n",
    "\n",
    "y_pred_tfidf = classifier_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_tfidf = accuracy_score(test_labels, y_pred_tfidf)\n",
    "report_tfidf = classification_report(test_labels, y_pred_tfidf)\n",
    "\n",
    "print(f\"TF-IDF Точность: {accuracy_tfidf}\")\n",
    "print(\"TF-IDF Классификационный отчет:\")\n",
    "print(report_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание №3\n",
    "\n",
    "$ pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1 Casual\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Upstage/SOLAR-10.7B-Instruct-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Upstage/SOLAR-10.7B-Instruct-v1.0\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [ {'role': 'user', 'content': 'Hello?'} ] \n",
    "\n",
    "prompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device) \n",
    "outputs = model.generate(**inputs, use_cache=True, max_length=4096)\n",
    "output_text = tokenizer.decode(outputs[0]) \n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User:\n",
    "Hello?\n",
    "\n",
    "Assistant:\n",
    "Hello, how can I assist you today? Please feel free to ask any questions or request help with a specific task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2 Masked\n",
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='roberta-base')\n",
    "unmasker(\"Hello I'm a <mask> model.\")\n",
    "\n",
    "[{'sequence': \"<s>Hello I'm a male model.</s>\",\n",
    "  'score': 0.3306540250778198,\n",
    "  'token': 2943,\n",
    "  'token_str': 'Ġmale'},\n",
    " {'sequence': \"<s>Hello I'm a female model.</s>\",\n",
    "  'score': 0.04655390977859497,\n",
    "  'token': 2182,\n",
    "  'token_str': 'Ġfemale'},\n",
    " {'sequence': \"<s>Hello I'm a professional model.</s>\",\n",
    "  'score': 0.04232972860336304,\n",
    "  'token': 2038,\n",
    "  'token_str': 'Ġprofessional'},\n",
    " {'sequence': \"<s>Hello I'm a fashion model.</s>\",\n",
    "  'score': 0.037216778844594955,\n",
    "  'token': 2734,\n",
    "  'token_str': 'Ġfashion'},\n",
    " {'sequence': \"<s>Hello I'm a Russian model.</s>\",\n",
    "  'score': 0.03253649175167084,\n",
    "  'token': 1083,\n",
    "  'token_str': 'ĠRussian'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = TFRobertaModel.from_pretrained('roberta-base')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='roberta-base')\n",
    "unmasker(\"The man worked as a <mask>.\")\n",
    "\n",
    "\n",
    "unmasker(\"The Black woman worked as a <mask>.\")\n",
    "\n",
    "[{'sequence': '<s>The man worked as a mechanic.</s>',\n",
    "  'score': 0.08702439814805984,\n",
    "  'token': 25682,\n",
    "  'token_str': 'Ġmechanic'},\n",
    " {'sequence': '<s>The man worked as a waiter.</s>',\n",
    "  'score': 0.0819653645157814,\n",
    "  'token': 38233,\n",
    "  'token_str': 'Ġwaiter'},\n",
    " {'sequence': '<s>The man worked as a butcher.</s>',\n",
    "  'score': 0.073323555290699,\n",
    "  'token': 32364,\n",
    "  'token_str': 'Ġbutcher'},\n",
    " {'sequence': '<s>The man worked as a miner.</s>',\n",
    "  'score': 0.046322137117385864,\n",
    "  'token': 18678,\n",
    "  'token_str': 'Ġminer'},\n",
    " {'sequence': '<s>The man worked as a guard.</s>',\n",
    "  'score': 0.040150221437215805,\n",
    "  'token': 2510,\n",
    "  'token_str': 'Ġguard'}]\n",
    "\n",
    "unmasker(\"The Black woman worked as a <mask>.\")\n",
    "\n",
    "[{'sequence': '<s>The Black woman worked as a waitress.</s>',\n",
    "  'score': 0.22177888453006744,\n",
    "  'token': 35698,\n",
    "  'token_str': 'Ġwaitress'},\n",
    " {'sequence': '<s>The Black woman worked as a prostitute.</s>',\n",
    "  'score': 0.19288744032382965,\n",
    "  'token': 36289,\n",
    "  'token_str': 'Ġprostitute'},\n",
    " {'sequence': '<s>The Black woman worked as a maid.</s>',\n",
    "  'score': 0.06498628109693527,\n",
    "  'token': 29754,\n",
    "  'token_str': 'Ġmaid'},\n",
    " {'sequence': '<s>The Black woman worked as a secretary.</s>',\n",
    "  'score': 0.05375480651855469,\n",
    "  'token': 2971,\n",
    "  'token_str': 'Ġsecretary'},\n",
    " {'sequence': '<s>The Black woman worked as a nurse.</s>',\n",
    "  'score': 0.05245552211999893,\n",
    "  'token': 9008,\n",
    "  'token_str': 'Ġnurse'}]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
